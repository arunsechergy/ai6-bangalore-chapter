{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fundamentals-of-neural-network_learnings.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/arunsechergy/ai6-bangalore-chapter/blob/master/session-4/notebook/fundamentals_of_neural_network_learnings.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "ttoZgJ3-2LNy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Session 4 Notes"
      ]
    },
    {
      "metadata": {
        "id": "ENV3Lpwx2PeS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**What is an artificial neuron?**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Simply put, it calculates a “weighted sum” of its input, adds a bias and then decides whether it should be “fired” or not ( yeah right, an activation function does this, but let’s go with the flow for a moment ).\n",
        "So consider a neuron.\n",
        "\n",
        "\n",
        "y = sum((weight * input) + bias )\n",
        "\n",
        "\n",
        "Backpropagation: https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c\n",
        "\n",
        "**Focused on ReLU**\n",
        "\n",
        "Synthesis:\n",
        "\t1. Different activation functions are used depending on the purpose of the hidden layer\n",
        "\t\ta. Above, the purpose of the first hidden layer is to make the input as non-linear input\n",
        "\t\tHence, ReLu[Rectifier Linear unit],\n",
        "\t\tA(x) = max(0,x)\n",
        "\t\tSource : From <https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0> \n",
        "\t\tAdvantages over other non-linear functions: \n",
        "\t\t\tReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations. \n",
        "\t\t\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "\t1. Because of the horizontal line in ReLu( for negative X ), the gradient can go towards 0. For activations in that region of ReLu, gradient will be 0 because of which the weights will not get adjusted during descent. \n",
        "\t2. That means, those neurons which go into that state will stop responding to variations in error/ input ( simply because gradient is 0, nothing changes ). This is called dying ReLu problem. \n",
        "\t3. This problem can cause several neurons to just die and not respond making a substantial part of the network passive. There are variations in ReLu to mitigate this issue by simply making the horizontal line into non-horizontal component . \n",
        "\t\ta. for example y = 0.01x for x<0 will make it a slightly inclined line rather than horizontal line. This is leaky ReLu. There are other variations too. The main idea is to let the gradient be non zero and recover during training eventually.\n",
        "\n",
        "\n",
        "\n",
        "**Loss Functions**:\n",
        "\n",
        "1.Cross Entropy loss = - [ylogy +(1-y) log(1-y)]\n",
        "\n",
        "   **Context**: we learn more slowly when our errors are less well-defined.\n",
        " \n",
        "    - Take the input 1 to the output 0. let's take a look at how the neuron learns.\n",
        "    - Initialize weight to be 0.6 and the bias to be 0.9 to start with\n",
        "    - Output from the neuron is 0.82 sigmoid([0.6*1 + 0.9])\n",
        "    - So quite a bit of learning will be needed before our neuron gets near the desired output, 0.0\n",
        "    - This slow learning is due to the very small partial derivatives. Hence, to make the partial derivatives big enough for the neural networks to learn faster when the error is very big, we introduce cross entropy loss, the log function in the cross entropy penalizes the big difference in the output. \n",
        "    \n",
        "\n",
        "*Reference*: 1. http://neuralnetworksanddeeplearning.com/chap3.html\n",
        "2. Activation functions: https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n"
      ]
    }
  ]
}